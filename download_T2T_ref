#!/bin/bash

# Base URL of the directories to download, without the trailing slash
BASE_URL="https://hgdownload.soe.ucsc.edu/gbdb/hs1"

# Directory where data should be downloaded
DOWNLOAD_DIR="../RawData"

# Ensure the download directory exists
mkdir -p "$DOWNLOAD_DIR"

# List of directories to download, only if they don't already exist and are not empty
ALL_DIRS=(encode gwasSNPs2022-03-08 hgCactus hgLiftOver hgUnique hoffmanMappability hs1.2bit html hubs ixIxx liftOver microsatellites ncbiRefSeq proseq rdnaModel rnaseq sedefSegDups sgdpCopyNumber t2tRepeatMasker)

# Loop through each directory and create a SLURM job to download it
for DIR in "${ALL_DIRS[@]}"; do
    LOCAL_DIR_PATH="${DOWNLOAD_DIR}/${DIR}"

    # Skip download if the directory exists and is not empty
    if [ -d "$LOCAL_DIR_PATH" ] && [ "$(ls -A "$LOCAL_DIR_PATH")" ]; then
        echo "Directory $DIR already exists and is not empty. Skipping..."
        continue
    fi

    # Create a SLURM script for the download job
    jobScript="wget_${DIR}.sh"
    cat > ${jobScript} << EOF
#!/bin/bash
#SBATCH --job-name=wget_$DIR
#SBATCH --output=wget_$DIR_%j.out
#SBATCH --time=02:00:00
#SBATCH --mem=2G
#SBATCH --ntasks=1

# Command to download the directory
wget -r -np -nH -P "$DOWNLOAD_DIR" -R "index.html*" "${BASE_URL}/${DIR}/"
EOF

    # Submit the job script to SLURM
    sbatch ${jobScript}
done

echo "Download jobs submitted!"
