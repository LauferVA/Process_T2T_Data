#!/bin/bash

# Base URL of the directories to download, without the trailing slash
BASE_URL="https://hgdownload.soe.ucsc.edu/gbdb/hs1"

# Directory where data should be downloaded
DOWNLOAD_DIR="../RawData"

# Ensure the download directory exists
mkdir -p "$DOWNLOAD_DIR"

# List of directories to download, only if they don't already exist and are not empty
ALL_DIRS=(bbi catLiftOffGenesV1 censat clinVar20220313 crispr cytoBandMapped dbSNP155 encode gwasSNPs2022-03-08 hgCactus hgLiftOver hgUnique hoffmanMappability hs1.2bit html hubs ixIxx liftOver microsatellites ncbiRefSeq proseq rdnaModel rnaseq sedefSeg$

# Loop through each directory and download it if it doesn't already exist and is not empty
for DIR in "${ALL_DIRS[@]}"; do
    # Construct the path to where the directory would be if downloaded
    LOCAL_DIR_PATH="${DOWNLOAD_DIR}/${DIR}"

    # Check if the directory exists and is not empty
    if [ -d "$LOCAL_DIR_PATH" ] && [ "$(ls -A "$LOCAL_DIR_PATH")" ]; then
        echo "Directory $DIR already exists and is not empty. Skipping..."
        continue
    fi

    echo "Downloading $DIR into $DOWNLOAD_DIR..."
    wget -r -np -nH -P "$DOWNLOAD_DIR" -R "index.html*" "${BASE_URL}/${DIR}/"
done

echo "All downloads completed!"
